{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Real_NVP (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d_UIHIUjw_PV"},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch\n","import cv2\n","import numpy as np\n","import torch.optim as optim\n","import torch.distributions as distributions\n","import torchvision\n","import torchvision.utils as utils \n","import torchvision.transforms as transforms\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLsMBBIgv2Kc"},"source":["%mkdir samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XjN7QfvNxHOE"},"source":["class res_block(torch.nn.Module):\n","    def __init__(self,dim):\n","        super(res_block,self).__init__()\n","        self.layer1=nn.BatchNorm2d(dim)\n","        self.residual_block_layer1=nn.utils.weight_norm(nn.Conv2d(dim,dim,(1,1),stride=1,padding=0,bias=False))\n","        #needs the scale weight to be fixed\n","        self.residual_block_layer2= nn.BatchNorm2d(dim)\n","        self.residual_block_layer3=nn.utils.weight_norm(nn.Conv2d(dim,dim,(3,3),stride=1,padding=1,bias=False))\n","        #needs the scale weight to be fixed\n","        self.residual_block_layer4=nn.BatchNorm2d(dim)\n","        self.residual_block_layer5=nn.utils.weight_norm(nn.Conv2d(dim,dim,(1,1),stride=1,padding=0,bias=True))\n","    \n","    def forward(self,x):\n","        self.residual_block_layer1.weight_g.data=torch.ones_like(self.residual_block_layer1.weight_g)\n","        self.residual_block_layer1.weight_g.requires_grad=False\n","\n","        \n","        self.residual_block_layer3.weight_g.data=torch.ones_like(self.residual_block_layer3.weight_g)\n","        self.residual_block_layer3.weight_g.requires_grad=False\n","\n","        op=self.layer1(x)\n","        op=F.relu(op)\n","        \n","        op=self.residual_block_layer1(op)\n","        op=self.residual_block_layer2(op)\n","        op=F.relu(op)\n","        \n","        op=self.residual_block_layer3(op)\n","        op=self.residual_block_layer4(op)\n","        op=F.relu(op)\n","        \n","        op=self.residual_block_layer5(op)\n","        return (x+op)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUfJbaVVxKLq","outputId":"24d77b93-f9f4-4d59-b5a8-6de65600a7d3","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["class residual_module(torch.nn.Module):\n","    def __init__(self,in_dim,res_dim,out_dim):\n","        super(residual_module,self).__init__()\n","        self.layer1_1=nn.utils.weight_norm(nn.Conv2d(in_dim,res_dim,(3,3),stride=1,padding=1,bias=True))\n","        #needs scale weight to be fixed\n","        self.layer1_2=res_block(res_dim)\n","        self.layer1_3=res_block(res_dim)\n","        self.layer1_4=res_block(res_dim)\n","        self.layer1_5=res_block(res_dim)\n","        self.layer1_6=nn.BatchNorm2d(res_dim)\n","        self.layer1_7=nn.utils.weight_norm(nn.Conv2d(res_dim,out_dim,(1,1),stride=1,padding=0,bias=True))\n","        \n","        self.layer2_1=nn.utils.weight_norm(nn.Conv2d(res_dim,res_dim,(1,1),stride=1,padding=0,bias=True))\n","        self.layer2_2=nn.utils.weight_norm(nn.Conv2d(res_dim,res_dim,(1,1),stride=1,padding=0,bias=True))\n","        self.layer2_3=nn.utils.weight_norm(nn.Conv2d(res_dim,res_dim,(1,1),stride=1,padding=0,bias=True))\n","        self.layer2_4=nn.utils.weight_norm(nn.Conv2d(res_dim,res_dim,(1,1),stride=1,padding=0,bias=True))\n","        self.layer2_5=nn.utils.weight_norm(nn.Conv2d(res_dim,res_dim,(1,1),stride=1,padding=0,bias=True))\n","    \n","    def forward(self,x):\n","        self.layer1_1.weight_g.data=torch.ones_like(self.layer1_1.weight_g)\n","        self.layer1_1.weight_g.requires_grad=False\n","\n","        x=self.layer1_1(x)\n","        x1=self.layer2_1(x)\n","        \n","        x=self.layer1_2(x)\n","        x1=x1+self.layer2_2(x)\n","        \n","        x=self.layer1_3(x)\n","        x1=x1+self.layer2_3(x)\n","        \n","        x=self.layer1_4(x)\n","        x1=x1+self.layer2_4(x)\n","        \n","        x=self.layer1_5(x)\n","        x1=x1+self.layer2_5(x)\n","        \n","        x=x1\n","        x=self.layer1_6(x)\n","        x=F.relu(x)\n","        \n","        op=self.layer1_7(x)\n","        return(op)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2dee8bfa41a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mresidual_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#needs scale weight to be fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","metadata":{"id":"yqAXNdlaxPgK"},"source":["def checkerboard_mask(config,size):\n","    if config == 1:\n","        mask=np.ones((size,size))\n","        mask[1::2,::2]=0\n","        mask[::2,1::2]=0\n","    else:\n","        mask=np.zeros((size,size))\n","        mask[1::2,::2]=1\n","        mask[::2,1::2]=1\n","    mask=mask.reshape(-1,1,size,size)\n","    return (torch.tensor(mask.astype(np.float32)))\n","\n","def mean_var(x):\n","  mean=torch.mean(x, dim=(0, 2, 3), keepdim=True)\n","  var=torch.mean((x - mean)**2, dim=(0, 2, 3), keepdim=True)#[E[x-E[x]]^2]\n","  return mean,var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZczJSmarfJc","outputId":"8126be41-8080-4943-b3d4-c6843aec5192","colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["p=nn.BatchNorm2d(5)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a38392907594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"reHVAzxDrmlc"},"source":["p.running_mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7mgTZxoxUeg"},"source":["class checkerboard_coupling(torch.nn.Module):\n","    def __init__(self,in_dim,mid_dim,out_dim,size,config):\n","        super(checkerboard_coupling,self).__init__()\n","        self.size=size\n","        self.config=config\n","        self.mask=checkerboard_mask(self.config,self.size).to(device)\n","        #try again without scale shifting the s output\n","        self.scale=torch.nn.Parameter(torch.zeros(1),requires_grad=True)\n","        self.scale_shift=torch.nn.Parameter((torch.zeros(1)),requires_grad=True)\n","        self.layer1=nn.BatchNorm2d(in_dim)\n","        self.layer2=residual_module(2*in_dim+1,mid_dim,2*out_dim)\n","        self.layer3=nn.BatchNorm2d(out_dim,affine=False)\n","        \n","        \n","    def forward(self,x):\n","        mask=self.mask.repeat(x.size(0),1,1,1)\n","        x1=self.layer1(x*mask)\n","        #offcial implementation does this,not sure why?\n","        x1=torch.cat((x1,-x1),dim=1)\n","        x1=torch.cat((x1,mask),dim=1)\n","        #####################################\n","        x1=F.relu(x1)\n","        res_op=self.layer2(x1)\n","        t,s=res_op.split(x.size(1),dim=1)\n","        s=self.scale*torch.tanh(s) + self.scale_shift\n","        #s=self.scale*torch.tanh(s)\n","        \n","        s=s*(1.-mask)\n","        t=t*(1.-mask)\n","        \n","        log_det_jacobian=s\n","        x=x*torch.exp(s) + t\n","        if self.training:\n","          #print(\"training_ch\")\n","          mean,var=mean_var(x)\n","        else:\n","          #print(\"val_ch\")\n","          var=self.layer3.running_var\n","          var=var.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        x=self.layer3(x) * (1. - mask) + x * mask\n","        log_det_jacobian=log_det_jacobian - 0.5 * torch.log(var + 1e-5) * (1. - mask)\n","        #try adding a batch-norm layer to output of the coupling_layer\n","        return (x,log_det_jacobian)\n","        \n","    def backward(self,x):\n","        mask=self.mask.repeat(x.size(0),1,1,1)\n","        x1=self.layer1(x*mask)\n","        #offcial implementation does this,not sure why?\n","        x1=torch.cat((x1,-x1),dim=1)\n","        x1=torch.cat((x1,mask),dim=1)\n","        #####################################\n","        x1=F.relu(x1)\n","        res_op=self.layer2(x1)\n","        t,s=res_op.split(x.size(1),dim=1)\n","        s=self.scale*torch.tanh(s) + self.scale_shift\n","        #s=self.scale*torch.tanh(s)\n","        \n","        s=s*(1.-mask)\n","        t=t*(1.-mask)\n","        \n","        log_det_jacobian=s\n","        mean,var=self.layer3.running_mean,self.layer3.running_var\n","        mean=mean.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        var=var.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        x = x * torch.exp(0.5 * torch.log(var + 1e-5) * (1. - mask)) + mean * (1. - mask)\n","        x=(x-t)*torch.exp(-s)\n","        #try adding a batch-norm layer to output of the coupling_layer \n","        return (x,log_det_jacobian)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ylatc-uWxWoq"},"source":["#The input to the channel-wise masking layer will be a sqeezed tensor with size=(b,c*4,h/2,w/2)    \n","class channel_coupling(torch.nn.Module):\n","    def __init__(self,in_dim,mid_dim,out_dim,config=1):\n","        super(channel_coupling,self).__init__()\n","        self.config=config\n","        self.scale=torch.nn.Parameter(torch.zeros(1),requires_grad=True)\n","        self.scale_shift=torch.nn.Parameter((torch.zeros(1)),requires_grad=True)\n","        self.layer1=nn.BatchNorm2d(in_dim//2)\n","        self.layer2=residual_module(in_dim,mid_dim,out_dim)\n","        self.layer3=nn.BatchNorm2d(int(in_dim/2),affine=False)\n","\n","        \n","    def forward(self,x):\n","        if self.config == 1:\n","            x1,x2=x.split(int(x.size(1)/2),dim=1)\n","        else:\n","            x2,x1=x.split(int(x.size(1)/2),dim=1)\n","        x_2=self.layer1(x2)\n","        x_2=torch.cat((x_2,-x_2),dim=1)\n","        x_2=F.relu(x_2)\n","        res_op=self.layer2(x_2)\n","        t,s=res_op.split(int(x.size(1)/2),dim=1)\n","        \n","        s=self.scale*torch.tanh(s) + self.scale_shift\n","        log_det_jacobian_tr=s\n","        \n","        x1=x1*torch.exp(s) + t\n","        if self.training:\n","          #print(\"training_chan\")\n","          mean,var=mean_var(x1)\n","        else:\n","          #print(\"val_chan\")\n","          var=self.layer3.running_var\n","          var=var.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        x1=self.layer3(x1)\n","        log_det_jacobian_tr=log_det_jacobian_tr - 0.5 * torch.log(var + 1e-5)\n","        if self.config == 1 :\n","            op=torch.cat((x1,x2),dim=1)\n","            #x1 is transformed x2 is left unchnaged\n","            log_det_jacobian=torch.cat((log_det_jacobian_tr,torch.zeros(log_det_jacobian_tr.shape).to(device)),dim=1)\n","        else:\n","            op=torch.cat((x2,x1),dim=1)\n","            #x1 is transformed x2 is left unchnaged\n","            log_det_jacobian=torch.cat((torch.zeros(log_det_jacobian_tr.shape).to(device),log_det_jacobian_tr),dim=1)\n","        return (op,log_det_jacobian)\n","    \n","    def backward(self,x):\n","        if self.config == 1:\n","            x1,x2=x.split(int(x.size(1)/2),dim=1)\n","        else:\n","            x2,x1=x.split(int(x.size(1)/2),dim=1)\n","        x_2=self.layer1(x2)\n","        x_2=torch.cat((x_2,-x_2),dim=1)\n","        x_2=F.relu(x_2)\n","        res_op=self.layer2(x_2)\n","        t,s=res_op.split(x.size(1)//2,dim=1)\n","        \n","        s=self.scale*torch.tanh(s) + self.scale_shift\n","        log_det_jacobian_tr=s\n","        \n","        mean, var = self.layer3.running_mean, self.layer3.running_var\n","        mean=mean.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        var=var.reshape(-1, 1, 1, 1).transpose(0, 1)\n","        x1=x1 * torch.exp(0.5 * torch.log(var + 1e-5)) + mean\n","        x1=(x1-t) * torch.exp(-s)\n","        if self.config == 1 :\n","            op=torch.cat((x1,x2),dim=1)\n","            #print(\"op:\",op.shape)\n","            #x1 is transformed x2 is left unchnaged\n","            log_det_jacobian=torch.cat((log_det_jacobian_tr,torch.zeros(log_det_jacobian_tr.shape).to(device)),dim=1)\n","        else:\n","            op=torch.cat((x2,x1),dim=1)\n","            #print(\"op:\",op.shape)\n","            #x1 is transformed x2 is left unchnaged\n","            log_det_jacobian=torch.cat((torch.zeros(log_det_jacobian_tr.shape).to(device),log_det_jacobian_tr),dim=1)\n","            #print(log_det_jacobian.shape)\n","        return (op,log_det_jacobian)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBnoYNqcxauJ"},"source":["class real_nvp(torch.nn.Module):\n","    def __init__(self,prior_dist,in_dim,mid_dim,out_dim,size):\n","        super(real_nvp,self).__init__()\n","        self.prior=prior_dist\n","        self.checkerboard_coupling_layer1=nn.ModuleList([checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=1),\n","                                                      checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=0),\n","                                                      checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=1)])\n","        \n","        self.channel_coupling_layer1=nn.ModuleList([channel_coupling(in_dim*4,mid_dim*2,out_dim*4,config=0),\n","                                             channel_coupling(in_dim*4,mid_dim*2,out_dim*4,config=1),\n","                                             channel_coupling(in_dim*4,mid_dim*2,out_dim*4,config=0)])\n","        \n","        self.order_matrix_1 = self.order_matrix(in_dim).to(device)\n","        #print(type(self.order_matrix_1))\n","\n","        in_dim=in_dim*2\n","        out_dim=out_dim*2\n","        mid_dim=mid_dim*2\n","        size=size//2\n","        \n","        self.checkerboard_coupling_layer2=nn.ModuleList([checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=1),\n","                                                         checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=0),\n","                                                         checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=1),\n","                                                         checkerboard_coupling(in_dim,mid_dim,out_dim,size,config=0)])\n","        \n","        \n","    def squeeze(self,x):\n","        [B, C, H, W] = list(x.size())\n","        x = x.reshape(B, C, H//2, 2, W//2, 2)\n","        x = x.permute(0, 1, 3, 5, 2, 4)\n","        x = x.reshape(B, C*4, H//2, W//2)\n","        return x\n","    \n","    def unsqueeze(self,x):\n","        b,c,h,w=x.size()\n","        x=x.reshape(b,c//4,2,2,h,w)\n","        x=x.permute(0,1,4,2,5,3)\n","        x=x.reshape(b,c//4,h*2,w*2)\n","        return (x)\n","    \n","    #Reference : https://github.com/tensorflow/models/blob/master/research/real_nvp/real_nvp_utils.py\n","    def order_matrix(self,channel):\n","        weights = np.zeros((channel*4, channel, 2, 2))\n","        ordering = np.array([[[[1., 0.],\n","                               [0., 0.]]],\n","                             [[[0., 0.],\n","                               [0., 1.]]],\n","                             [[[0., 1.],\n","                               [0., 0.]]],\n","                             [[[0., 0.],\n","                               [1., 0.]]]])\n","        for i in range(channel):\n","            s1 = slice(i, i+1)\n","            s2 = slice(4*i, 4*(i+1))\n","            weights[s2, s1, :, :] = ordering\n","        shuffle = np.array([4*i for i in range(channel)]\n","                         + [4*i+1 for i in range(channel)]\n","                         + [4*i+2 for i in range(channel)]\n","                         + [4*i+3 for i in range(channel)])\n","        weights = weights[shuffle, :, :, :].astype('float32')\n","        return torch.tensor(weights)\n","    \n","    \n","    def inference(self,x):\n","        #for every scale 3 checkerboard coupling layer ==> squeeze tensor ==> channel_masking ==> unsqueeze ==>factor out half \n","        #the dimensions \n","        #scale 1\n","        z=x\n","        log_det_jacobian=torch.zeros(z.shape).to(device)\n","        for i in range(len(self.checkerboard_coupling_layer1)):\n","            z,jacobian=self.checkerboard_coupling_layer1[i].forward(z)\n","            log_det_jacobian+=jacobian\n","\n","        z=self.squeeze(z)\n","        log_det_jacobian=self.squeeze(log_det_jacobian)\n","\n","        for i in range(len(self.channel_coupling_layer1)):\n","            z,jacobian=self.channel_coupling_layer1[i].forward(z)\n","            log_det_jacobian=log_det_jacobian + jacobian\n","\n","        #print(type(z))\n","        z=self.unsqueeze(z)\n","        log_det_jacobian=self.unsqueeze(log_det_jacobian)\n","\n","        #print(type(z))\n","        y=F.conv2d(z,self.order_matrix_1,stride=2,padding=0)\n","        z,z1=y.split(int(y.size(1)/2),dim=1)\n","\n","        y=F.conv2d(log_det_jacobian,self.order_matrix_1,stride=2,padding=0)\n","        log_det_jacobian,log_det_jacobian1=y.split(int(y.size(1)/2),dim=1)\n","\n","        #scale 2\n","        for i in range(len(self.checkerboard_coupling_layer2)):\n","            z,jacobian=self.checkerboard_coupling_layer2[i].forward(z)\n","            log_det_jacobian+=jacobian\n","\n","        final_op=torch.cat((z,z1),dim=1)\n","        final_op=F.conv_transpose2d(final_op,self.order_matrix_1,stride=2, padding=0)\n","\n","        final_det_jacobian=torch.cat((log_det_jacobian,log_det_jacobian1),dim=1)\n","        final_det_jacobian=F.conv_transpose2d(final_det_jacobian,self.order_matrix_1,stride=2, padding=0)\n","\n","        return (final_op,final_det_jacobian)\n","    \n","    def sampling(self,z):\n","        #\n","        #reverse order of infernece \n","        y=F.conv2d(z,self.order_matrix_1,stride=2,padding=0)\n","        x,x1=y.split(int(y.size(1)/2),dim=1)\n","        \n","        for coup_layer in self.checkerboard_coupling_layer2[::-1]:\n","            x,_=coup_layer.backward(x)\n","        \n","        x=torch.cat((x,x1),dim=1)\n","        x=F.conv_transpose2d(x,self.order_matrix_1,stride=2,padding=0)\n","        \n","        x=self.squeeze(x)\n","        \n","        for coup_layer in self.channel_coupling_layer1[::-1]:\n","            x,_=coup_layer.backward(x)\n","        \n","        x=self.unsqueeze(x)\n","        for coup_layer in self.checkerboard_coupling_layer1[::-1]:\n","            x,_=coup_layer.backward(x)\n","        \n","        return (x)\n","    \n","    def likelihood(self,x):\n","        #log(p(x))=log(ph(f(x))+log(sii)\n","        x_,det_jacobian=self.inference(x)\n","        assert (x_.shape == det_jacobian.shape)\n","        det_jacobian=torch.sum(det_jacobian,dim=(1,2,3))\n","        log_likelihood=torch.sum(self.prior.log_prob(x_),dim=(1,2,3))\n","        return(log_likelihood+det_jacobian)\n","    \n","    def sample_images(self,number,channel,width,heigth):\n","        z=self.prior.sample((number,channel,width,heigth))\n","        x=self.sampling(z)\n","        return (x)\n","    \n","    def forward(self,x):\n","        weight_scale=0\n","        o=[]\n","        for name,parameter in self.named_parameters():\n","            p_name=name.split(\".\")[-1]\n","            if (p_name == \"weight_g\" or p_name == \"scale\") and parameter.requires_grad==True:\n","                weight_scale+=torch.pow(parameter,2).sum()\n","        x_ll=self.likelihood(x)\n","        return (x_ll,weight_scale)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0yr9xvkxc4S"},"source":["mean=torch.tensor(0.).to(device)\n","std_dev=torch.tensor(1.).to(device)\n","prior_dist=distributions.Normal(mean,std_dev)\n","#prior_dist=prior_dist.to(device)\n","r=torch.randn(8,1,28,28).to(device)\n","x=real_nvp(prior_dist,1,64,1,28).to(device)\n","print(\"ll\",x.forward(r)[0].shape)\n","print(\"generated_images\",x.sample_images(5,1,28,28).shape)\n","plt.imshow(x.sample_images(5,1,28,28)[0,0,:,:].cpu().detach().numpy(),cmap='gray')\n","print(x.sample_images(5,1,28,28).dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfrhhLGSyrdL"},"source":["transforms_train = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.ToTensor()])\n","transforms_val=transforms.ToTensor()\n","batch_size=32\n","training_data=torchvision.datasets.CIFAR10(root='torch/data/cifar10',train=True, download=True, transform=transforms_train)\n","train_loader=torch.utils.data.DataLoader(training_data,batch_size=batch_size, shuffle=True, num_workers=8)\n","\n","validation_data=torchvision.datasets.CIFAR10(root='torch/data/cifar10',train=False,download=True,transform=transforms_val)\n","val_loader=torch.utils.data.DataLoader(validation_data,batch_size=batch_size,shuffle=True,num_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-S3bBct2teP"},"source":["mean=torch.tensor(0.).to(device)\n","std_dev=torch.tensor(1.).to(device)\n","prior_dist=distributions.Normal(mean,std_dev)\n","flow_model=real_nvp(prior_dist,in_dim=3,mid_dim=64,out_dim=3,size=32).to(device)\n","momentum=0.9\n","decay=0.999\n","lr=1e-3\n","optimizer=optim.Adamax(flow_model.parameters(),lr=lr,betas=(momentum,decay), eps=1e-7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6A3usXyLeQ6F"},"source":[""]},{"cell_type":"code","metadata":{"id":"zaR5P3rn3IfK"},"source":["print(r.shape)\n","noise=torch.distributions.Uniform(0,1).sample(r.shape)\n","print(noise.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iN-GP8_VPiiA"},"source":["max_epoch=5000\n","scale_reg=5e-5\n","epoch=0\n","avg_loss=0.\n","avg_likelihood=0.\n","while epoch < max_epoch:\n","  epoch=epoch+1\n","  print(\"Epoch np:\",epoch)\n","  flow_model.train()\n","  for  batch_idx, data in enumerate(train_loader):\n","    optimizer.zero_grad()\n","    img,label=data[0],data[1]\n","    noise = distributions.Uniform(0., 1.).sample(img.shape)\n","    img=(img *255.+noise) /256.\n","    img*= 2.             # [0, 2]\n","    img-= 1.             # [-1, 1]\n","    img*=0.9     # [-0.9, 0.9]\n","    img+= 1.             # [0.1, 1.9]\n","    img/=2.             # [0.05, 0.95]\n","    logit_img=torch.log(img) - torch.log(1. - img)\n","    pre_logit_scale=torch.tensor(np.log(0.9) - np.log(1. - 0.9))\n","    log_jacobian=F.softplus(logit_img) + F.softplus(-logit_img)-F.softplus(-pre_logit_scale)\n","    log_jacobian=torch.sum(log_jacobian,dim=(1,2,3))\n","    img=torch.log(img)-torch.log(1.-img)\n","    img=img.to(device)\n","    log_jacobian=log_jacobian.to(device)\n","\n","    batch_likelihood,weight_scale=flow_model.forward(img)\n","    # print(batch_likelihood.shape)\n","    # print(log_jacobian.shape)\n","    log_likelihood = (batch_likelihood + log_jacobian).mean()\n","\n","    loss=-log_likelihood + scale_reg*weight_scale\n","    avg_loss+=loss.item()\n","    avg_likelihood+=log_likelihood.item()\n","    \n","    loss.backward()\n","    optimizer.step()\n","\n","    if batch_idx% 1000 == 0:\n","      # print('[%d/%d]\\tloss: %.3f\\tlog-ll: %.3f' % \\\n","      #               (batch_idx*batch_size, len(train_loader.dataset), \n","      #                   loss.item(), log_ll.item(), bit_per_dim))\n","      print(\"[%d/%d]\\tloss: %.3f\\tlog-ll: %.3f\" % (batch_idx*batch_size, len(train_loader.dataset),loss.item(),log_likelihood.item()))\n","\n","  mean_loss=avg_loss/batch_idx\n","  mean_likelihood=avg_likelihood/batch_idx\n","\n","  print('===> Average train loss: %.3f' % mean_loss)\n","  print('===> Average train log-likelihood: %.3f' % mean_likelihood)\n","  avg_loss=0\n","  avg_likelihood=0\n","\n","  flow_model.eval()\n","  with torch.no_grad():\n","    for  batch_idx, data in enumerate(val_loader):\n","      #optimizer.zero_grad()\n","      img,label=data[0],data[1]\n","      noise = distributions.Uniform(0., 1.).sample(img.shape)\n","      img=(img *255.+noise) /256.\n","      img*= 2.             # [0, 2]\n","      img-= 1.             # [-1, 1]\n","      img*=0.9     # [-0.9, 0.9]\n","      img+= 1.             # [0.1, 1.9]\n","      img/=2.             # [0.05, 0.95]\n","      logit_img=torch.log(img) - torch.log(1. - img)\n","      pre_logit_scale=torch.tensor(np.log(0.9) - np.log(1. - 0.9))\n","      log_jacobian=F.softplus(logit_img) + F.softplus(-logit_img)-F.softplus(-pre_logit_scale)\n","      log_jacobian=torch.sum(log_jacobian,dim=(1,2,3))\n","      img=torch.log(img)-torch.log(1.-img)\n","      img=img.to(device)\n","      log_jacobian=log_jacobian.to(device)\n","\n","      batch_likelihood,weight_scale=flow_model.forward(img)\n","      # print(batch_likelihood.shape)\n","      # print(log_jacobian.shape)\n","      log_likelihood = (batch_likelihood + log_jacobian).mean()\n","\n","      loss=-log_likelihood + scale_reg*weight_scale\n","      avg_loss+=loss.item()\n","      avg_likelihood+=log_likelihood.item()\n","    mean_loss=avg_loss/batch_idx\n","    mean_likelihood=avg_likelihood/batch_idx\n","\n","    print('===> Average validation loss: %.3f' % mean_loss)\n","    print('===> Average validation log-likelihood: %.3f' % mean_likelihood)\n","    avg_loss=0\n","    avg_likelihood=0\n","\n","\n","    gen_images=flow_model.sample_images(64,3,32,32)\n","    gen_images=1./(torch.exp(-gen_images) + 1.)    # [0.05, 0.95]\n","    gen_images*=2.             # [0.1, 1.9]\n","    gen_images-=1.             # [-0.9, 0.9]\n","    gen_images/=0.9     # [-1, 1]\n","    gen_images+=1.             # [0, 2]\n","    gen_images/=2.             # [0, 1]\n","    \n","    print(gen_images.dtype)\n","    print(type(gen_images))\n","    utils.save_image(utils.make_grid(torch.tensor(gen_images.clone().detach())),'./samples/'+'_ep%d.png' % epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCNYankzhqGB"},"source":[""],"execution_count":null,"outputs":[]}]}